# -*- coding: utf-8 -*-
"""Titanic_survival.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UzOZxgT9KQvmmSSZ7oxQYDsYYtaM6LY-
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Titanic dataset read karna
df = pd.read_csv("titanic.csv")

# First 5 rows dekhna
df.head()

# Dataset info check karna
df.info()

# Age column ke null values fill with median
df["Age"].fillna(df["Age"].median(), inplace=True)

# Check again
df["Age"].isnull().sum()

# Fare column ke null values fill with median
df["Fare"].fillna(df["Fare"].median(), inplace=True)

# Check again
df["Fare"].isnull().sum()

# Cabin column drop kar dena (bohot zyada missing values)
df.drop("Cabin", axis=1, inplace=True)

# Check again
df.isnull().sum()

# Sex column encode karna (male = 0, female = 1)
df["Sex"] = df["Sex"].map({"male": 0, "female": 1})

# Check changes
df[["Sex"]].head()

# Embarked column encode karna (S=0, C=1, Q=2)
df["Embarked"] = df["Embarked"].map({"S": 0, "C": 1, "Q": 2})

# Check changes
df[["Embarked"]].head()

# Extra columns drop karna
df.drop(["PassengerId", "Name", "Ticket"], axis=1, inplace=True)

# Check remaining columns
df.head()

# Step 1: Dataset Overview

# Shape of dataset (rows, columns)
print("Dataset Shape:", df.shape)

# Basic info about dataset
print("\nDataset Info:")
print(df.info())

# Statistical summary
print("\nStatistical Summary:")
print(df.describe())

plt.figure(figsize=(6,4))
sns.countplot(x="Survived", data=df, palette="viridis")

plt.title("Overall Survival Count", fontsize=14)
plt.xlabel("Survived (0 = No, 1 = Yes)")
plt.ylabel("Count")
plt.show()

# Step 3: Gender-wise Survival

plt.figure(figsize=(6,4))
sns.countplot(x="Sex", hue="Survived", data=df, palette="viridis")

plt.title("Survival Count by Gender", fontsize=14)
plt.xlabel("Sex (0 = Male, 1 = Female)")
plt.ylabel("Count")
plt.legend(title="Survived", labels=["No (0)", "Yes (1)"])
plt.show()

# Step 4: Pclass-wise Survival

plt.figure(figsize=(6,4))
sns.countplot(x="Pclass", hue="Survived", data=df, palette="viridis")

plt.title("Survival Count by Passenger Class", fontsize=14)
plt.xlabel("Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)")
plt.ylabel("Count")
plt.legend(title="Survived", labels=["No (0)", "Yes (1)"])
plt.show()

# Step 5: Age Distribution with Survival

plt.figure(figsize=(8,5))
sns.histplot(data=df, x="Age", hue="Survived", bins=30, kde=True, palette="viridis")

plt.title("Age Distribution by Survival", fontsize=14)
plt.xlabel("Age")
plt.ylabel("Count")
plt.show()

# Step 6: Fare Distribution with Survival

plt.figure(figsize=(8,5))
sns.boxplot(x="Survived", y="Fare", data=df, palette="viridis")

plt.title("Fare Distribution by Survival", fontsize=14)
plt.xlabel("Survived (0 = No, 1 = Yes)")
plt.ylabel("Fare")
plt.show()

# Step 6: Fare Distribution with Survival

plt.figure(figsize=(8,5))
sns.boxplot(x="Survived", y="Fare", data=df, palette="viridis")

plt.title("Fare Distribution by Survival", fontsize=14)
plt.xlabel("Survived (0 = No, 1 = Yes)")
plt.ylabel("Fare")
plt.show()

# Fare distribution with survival (Histogram)

plt.figure(figsize=(8,5))
sns.histplot(data=df, x="Fare", hue="Survived", bins=30, kde=True, palette="viridis")

plt.title("Fare Distribution by Survival", fontsize=14)
plt.xlabel("Fare")
plt.ylabel("Count")
plt.xlim(0, 100)  # Limit rakha hai taake extreme outliers na kharab karein
plt.show()

# Step 7: Embarked vs Survival

plt.figure(figsize=(6,4))
sns.countplot(x="Embarked", hue="Survived", data=df, palette="viridis")

plt.title("Survival Count by Embarked Port", fontsize=14)
plt.xlabel("Embarked (0 = S, 1 = C, 2 = Q)")
plt.ylabel("Count")
plt.legend(title="Survived", labels=["No (0)", "Yes (1)"])
plt.show()

# Step 8A: SibSp vs Survival
plt.figure(figsize=(6,4))
sns.countplot(x="SibSp", hue="Survived", data=df, palette="Set2")

plt.title("Survival Count by SibSp", fontsize=14)
plt.xlabel("Number of Siblings/Spouses Aboard")
plt.ylabel("Count")
plt.legend(title="Survived", labels=["No (0)", "Yes (1)"])
plt.show()

# Step 8B: Parch vs Survival
plt.figure(figsize=(6,4))
sns.countplot(x="Parch", hue="Survived", data=df, palette="Set1")

plt.title("Survival Count by Parch", fontsize=14)
plt.xlabel("Number of Parents/Children Aboard")
plt.ylabel("Count")
plt.legend(title="Survived", labels=["No (0)", "Yes (1)"])
plt.show()

# Step 9: Correlation Heatmap

plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)

plt.title("Correlation Heatmap of Numerical Features", fontsize=14)
plt.show()

# Step 10: Pairplot for Overall EDA

sns.pairplot(
    df[["Survived", "Pclass", "Age", "Fare", "SibSp", "Parch"]],
    hue="Survived",
    palette="husl",
    diag_kind="kde"
)
plt.suptitle("Pairplot of Titanic Dataset (Survival vs Features)", y=1.02, fontsize=16)
plt.show()

from sklearn.model_selection import train_test_split

# Features (X) and Target (y)
X = df.drop("Survived", axis=1)   # input features
y = df["Survived"]                # target (0 or 1)

# Split dataset into train (80%) and test (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Step 1: Model define
log_reg = LogisticRegression(max_iter=1000)

# Step 2: Train
log_reg.fit(X_train, y_train)

# Step 3: Predict
y_pred = log_reg.predict(X_test)

# Step 4: Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

from sklearn.tree import DecisionTreeClassifier

# Step 1: Model define
dt = DecisionTreeClassifier(random_state=42)

# Step 2: Train
dt.fit(X_train, y_train)

# Step 3: Predict
y_pred_dt = dt.predict(X_test)

# Step 4: Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))

from sklearn.ensemble import RandomForestClassifier

# Step 1: Model define
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Step 2: Train
rf.fit(X_train, y_train)

# Step 3: Predict
y_pred_rf = rf.predict(X_test)

# Step 4: Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

from sklearn.model_selection import cross_val_score

# Random Forest cross-validation
cv_scores = cross_val_score(rf, X, y, cv=5)  # 5-fold CV

print("Cross-Validation Scores:", cv_scores)
print("Mean CV Accuracy:", cv_scores.mean())

import matplotlib.pyplot as plt
import seaborn as sns

# Feature importance from Random Forest
importances = rf.feature_importances_
feature_names = X.columns

# Plot
plt.figure(figsize=(8,5))
sns.barplot(x=importances, y=feature_names, palette="viridis")
plt.title("Feature Importance (Random Forest)", fontsize=14)
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

import joblib


joblib.dump(rf, r"C:\Users\Lenovo\Desktop\titanic\titanic_model.pkl")